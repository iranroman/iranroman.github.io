<p align="right"><i>
progress is imminent when we share with each other
</i></p>

Throughout my academic journey, I've delved into the intricacies of human perception and action, translating these insights into machine perception models. As a postdoctoral scholar at [New York University's Tandon School of Engineering](https://cusp.nyu.edu/profiles/iran-roman/) and the [Music and Audio Research Laboratory](https://steinhardt.nyu.edu/people/iran-r-roman), I explore the nexus between human behavior and AI. With a foundation in biology, signal processing, and linguistics, I challenge conventional views of human behavior to discover mechanisms that can inform novel insights for AI models.

My research has been particularly drawn to human actions with rhythmic events. Traditional Bayesian models fall short in capturing this, leading me to pioneer models anchored in “strong anticipation”. These non-linear oscillator-based models illuminate the behavioral nuances between musicians and non-musicians, a contribution recognized by Stanford’s [Human-Centered Artificial Intelligence award](https://stanforddaily.com/2019/05/01/stanfords-human-centered-ai-institute-awards-30-seed-grants/).

In the realm of signal processing, I aim to enhance machine learning models for spatial perception, particularly in Sound Event Localization and Detection. By harnessing acoustic imaging and computer vision, I've advanced models for sound source distance and direction of arrival estimation. My contributions extend to open-source projects like `librosa`, `soundata`, `mirdata`, and `micarraylib`. Parallel to academia, my research has found applications in product development at renowned companies such as Apple, Tesla, Raytheon/BBN, and Plantronics.
